{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "As we've seen before with Q learning, it's all about generating an episode and updating the Q-values using the max estimate from the next state $S'$:\n",
    "\n",
    "$$  \\begin{align*}\n",
    "    Q(s,a) = Q(s,a) + alpha \\cdot (r_{t+1} + argmax_{a'}Q(s',a') - Q(s,a))\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "So, most of the code of this section will be exactly the same as the previous one. The only thing that will change is going to be the loss function.\n",
    "\n",
    "Remember that this is an off-policy algorithm because the episodes and q-values are updated using different policies. For the former task, the policy will be allowed to both explore and exploit, but the latter will only be able to exploit."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
